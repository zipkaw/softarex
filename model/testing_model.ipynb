{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class Model1KNN:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(tr_x)\n",
    "        tr_x = self.scaler.transform(tr_x)\n",
    "        #params = {\"kernel\":['rbf'],\"C\":np.logspace(0,1,params_cnt), \"epsilon\":np.logspace(-1,1,params_cnt)}\n",
    "        self.model = KNeighborsRegressor(n_neighbors=5,\n",
    "                                         #weights='uniform'\n",
    "                                        )\n",
    "        \n",
    "        self.model.fit(tr_x,tr_y)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        x = self.scaler.transform(x)\n",
    "        pred = self.model.predict(x)\n",
    "        return pred\n",
    "    \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "class Model1NN:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "    '''\n",
    "    def weight_variable(self,shape,name):\n",
    "        initial =tf.truncated_normal(shape,stddev=0.1)\n",
    "        return tf.Variable(initial, name=name)\n",
    "\n",
    "    def bias_variable(self,shape,name):\n",
    "        initial = tf.constant(0.1,shape=shape)\n",
    "        return tf.Variable(initial, name=name)\n",
    "    '''     \n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(tr_x)\n",
    "        \n",
    "        batch_size = 128\n",
    "        epochs = 10000\n",
    "        \n",
    "        tr_x = self.scaler.transform(tr_x)\n",
    "        va_x = self.scaler.transform(va_x)\n",
    "        \n",
    "        early_stopping =  EarlyStopping(\n",
    "                            monitor='val_loss',\n",
    "                            min_delta=0.0,\n",
    "                            patience=20,\n",
    "        )\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_shape=(tr_x.shape[1],)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(32, activation='relu'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='mean_squared_error', #'categorical_crossentropy',#categorical_crossentropy\n",
    "                      optimizer='adam')\n",
    "\n",
    "        history = model.fit(tr_x, tr_y,\n",
    "                            batch_size=batch_size, epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=(va_x, va_y),\n",
    "                            callbacks=[early_stopping])\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.scaler.transform(x)\n",
    "        pred = self.model.predict(x).argmax(axis=1)\n",
    "        return pred\n",
    "    \n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "class Model1lgb:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        lgb_params = {'objective': 'rmse',\n",
    "                  'random_state': 10,\n",
    "                  'metric': 'rmse'}\n",
    "        lgb_train = lgb.Dataset(tr_x, label=tr_y)\n",
    "        lgb_eval = lgb.Dataset(va_x, label=va_y,reference=lgb_train)\n",
    "        self.model = lgb.train(lgb_params, lgb_train, valid_sets=lgb_eval, num_boost_round=10000,early_stopping_rounds=50)\n",
    "\n",
    "    def predict(self, x):\n",
    "        pred = self.model.predict(x,num_iteration=self.model.best_iteration)\n",
    "        return pred\n",
    "    \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "class Model1RF:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(tr_x)\n",
    "        tr_x = self.scaler.transform(tr_x)\n",
    "        self.model = RandomForestRegressor(\n",
    "            max_depth=5,\n",
    "            n_estimators=100,\n",
    "            random_state=10,\n",
    "        )\n",
    "        self.model.fit(tr_x,tr_y)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        x = self.scaler.transform(x)\n",
    "        pred = self.model.predict(x)\n",
    "        return pred\n",
    "    \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class Model2Linear:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, tr_x, tr_y, va_x, va_y):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(tr_x)\n",
    "        tr_x = self.scaler.transform(tr_x)\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(tr_x, tr_y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = self.scaler.transform(x)\n",
    "        pred = self.model.predict(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train, test):\n",
    "    df_trainval = pd.read_csv(train)\n",
    "    df_test = pd.read_csv(test)\n",
    "\n",
    "    y_trainval = df_trainval['revenue']\n",
    "    del df_trainval['revenue']\n",
    "\n",
    "    df_all = pd.concat([df_trainval,df_test],axis=0)\n",
    "    df_all['Open Date'] = pd.to_datetime(df_all[\"Open Date\"])\n",
    "    df_all['Year'] = df_all['Open Date'].apply(lambda x:x.year)\n",
    "    df_all['Month'] = df_all['Open Date'].apply(lambda x:x.month)\n",
    "    df_all['Day'] = df_all['Open Date'].apply(lambda x:x.day)\n",
    "    df_all['week_name'] = df_all['Open Date'].apply(lambda x:x.day_name())\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df_all['City'] = le.fit_transform(df_all['City'])\n",
    "    df_all['City Group'] = df_all['City Group'].map({'Other':0,'Big Cities':1}) #There are only 'Other' or 'Big city'\n",
    "    df_all[\"Type\"] = df_all[\"Type\"].map({\"FC\":0, \"IL\":1, \"DT\":2, \"MB\":3}) #There are only 'FC' or 'IL' or 'DT' or 'MB'\n",
    "    df_all[\"week_name\"] = df_all[\"week_name\"].map({\"Sunday\":0, \"Monday\":1, \"Tuesday\":2, \"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6}) #There are only 'FC' or 'IL' or 'DT' or 'MB'\n",
    "\n",
    "    df_trainval = df_all.iloc[:df_trainval.shape[0]]\n",
    "\n",
    "    df_test = df_all.iloc[df_trainval.shape[0]:]\n",
    "    df_train_col = [col for col in df_trainval.columns if col not in ['Id','Open Date']]\n",
    "    df_trainval = df_trainval[df_train_col]\n",
    "    df_test = df_test[df_train_col]\n",
    "    print(df_test)\n",
    "    print(df_trainval)\n",
    "    return df_trainval, y_trainval, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(test):\n",
    "    df_test = pd.read_csv(test)\n",
    "\n",
    "    df_all = pd.concat([df_test],axis=0)\n",
    "    df_all['Open Date'] = pd.to_datetime(df_all[\"Open Date\"])\n",
    "    df_all['Year'] = df_all['Open Date'].apply(lambda x:x.year)\n",
    "    df_all['Month'] = df_all['Open Date'].apply(lambda x:x.month)\n",
    "    df_all['Day'] = df_all['Open Date'].apply(lambda x:x.day)\n",
    "    df_all['week_name'] = df_all['Open Date'].apply(lambda x:x.day_name())\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df_all['City'] = le.fit_transform(df_all['City'])\n",
    "    df_all['City Group'] = df_all['City Group'].map({'Other':0,'Big Cities':1}) #There are only 'Other' or 'Big city'\n",
    "    df_all[\"Type\"] = df_all[\"Type\"].map({\"FC\":0, \"IL\":1, \"DT\":2, \"MB\":3}) #There are only 'FC' or 'IL' or 'DT' or 'MB'\n",
    "    df_all[\"week_name\"] = df_all[\"week_name\"].map({\"Sunday\":0, \"Monday\":1, \"Tuesday\":2, \"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6}) #There are only 'FC' or 'IL' or 'DT' or 'MB'\n",
    "    df_test = df_all\n",
    "    df_train_col = [col for col in df_test.columns if col not in ['Id','Open Date']]\n",
    "    df_test = df_test[df_train_col]\n",
    "    print(df_test)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(train):\n",
    "    df_trainval = pd.read_csv(train)\n",
    "\n",
    "    y_trainval = df_trainval['revenue']\n",
    "    del df_trainval['revenue']\n",
    "\n",
    "    df_all = pd.concat([df_trainval],axis=0)\n",
    "    df_all['Open Date'] = pd.to_datetime(df_all[\"Open Date\"])\n",
    "    df_all['Year'] = df_all['Open Date'].apply(lambda x:x.year)\n",
    "    df_all['Month'] = df_all['Open Date'].apply(lambda x:x.month)\n",
    "    df_all['Day'] = df_all['Open Date'].apply(lambda x:x.day)\n",
    "    df_all['week_name'] = df_all['Open Date'].apply(lambda x:x.day_name())\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    df_all['City'] = le.fit_transform(df_all['City'])\n",
    "    df_all['City Group'] = df_all['City Group'].map({'Other':0,'Big Cities':1}) #There are only 'Other' or 'Big city'\n",
    "    df_all[\"Type\"] = df_all[\"Type\"].map({\"FC\":0, \"IL\":1, \"DT\":2, \"MB\":3}) #There are only 'FC' or 'IL' or 'DT' or 'MB'\n",
    "    df_all[\"week_name\"] = df_all[\"week_name\"].map({\"Sunday\":0, \"Monday\":1, \"Tuesday\":2, \"Wednesday\":3,\"Thursday\":4,\"Friday\":5,\"Saturday\":6}) #There are only 'FC' or 'IL' or 'DT' or 'MB'\n",
    "\n",
    "    df_trainval = df_all.iloc[:df_trainval.shape[0]]\n",
    "    df_train_col = [col for col in df_trainval.columns if col not in ['Id','Open Date']]\n",
    "    df_trainval = df_trainval[df_train_col]\n",
    "    print(df_trainval)\n",
    "    return df_trainval, y_trainval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_and_predict(model, train_x, train_y):\n",
    "    preds = []\n",
    "    va_idxes = []\n",
    "\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=10)\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
    "        tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "        model.fit(tr_x, tr_y, va_x, va_y)\n",
    "        pred = model.predict(va_x)\n",
    "        preds.append(pred)\n",
    "        va_idxes.append(va_idx)\n",
    "\n",
    "    va_idxes = np.concatenate(va_idxes)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    order = np.argsort(va_idxes)\n",
    "    pred_train = preds[order]\n",
    "\n",
    "    return pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_x):\n",
    "    return model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitting_models(models:dict, df_trainval, y_trainval):\n",
    "    predict_train = []\n",
    "    lin = models.pop('Linear')\n",
    "    for model in models.values():\n",
    "        predict_train.append(train_model_and_predict(model, df_trainval, y_trainval))\n",
    "    pred_dict = {}\n",
    "    for i, sublists in enumerate(predict_train, start=1):\n",
    "        pred_dict[i] = sublists\n",
    "\n",
    "    train_x_2 = pd.DataFrame(pred_dict)\n",
    "    train_model_and_predict(lin, train_x_2, y_trainval)\n",
    "    return lin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_revenue(final_regressor, models:dict, df_test):\n",
    "    predict_test = []\n",
    "    \n",
    "    for model in models.values():\n",
    "        predict_test.append(predict(model, df_test))\n",
    "    pred_dict = {}\n",
    "    for i, sublists in enumerate(predict_test, start=1):\n",
    "        pred_dict[i] = sublists\n",
    "\n",
    "    test_x_2 = pd.DataFrame(pred_dict)\n",
    "    return predict(final_regressor, test_x_2)\n",
    "\n",
    "def save_to_cvs(prediction):\n",
    "    submission = pd.DataFrame({'Prediction':prediction})\n",
    "    submission.to_csv('./submission4.csv',index=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     City  City Group  Type  P1   P2   P3   P4  P5  P6  P7  ...  P32  P33   \n",
      "0      31           1     1   4  5.0  4.0  4.0   2   2   5  ...    4    5  \\\n",
      "1       3           1     0   4  5.0  4.0  4.0   1   2   5  ...    0    0   \n",
      "2      10           0     1   2  4.0  2.0  5.0   2   3   5  ...    0    0   \n",
      "3      28           0     1   6  4.5  6.0  6.0   4   4  10  ...   10    6   \n",
      "4      14           0     1   3  4.0  3.0  4.0   2   2   5  ...    3    2   \n",
      "..    ...         ...   ...  ..  ...  ...  ...  ..  ..  ..  ...  ...  ...   \n",
      "132    29           0     0   2  3.0  3.0  5.0   4   2   4  ...    0    0   \n",
      "133    32           1     0   4  5.0  4.0  4.0   2   3   5  ...    0    0   \n",
      "134    18           0     0   3  4.0  4.0  4.0   2   3   5  ...    0    0   \n",
      "135    31           1     0   4  5.0  4.0  5.0   2   2   5  ...    0    0   \n",
      "136    31           1     0   4  5.0  3.0  5.0   2   2   5  ...    0    0   \n",
      "\n",
      "     P34  P35  P36  P37  Year  Month  Day  week_name  \n",
      "0      5    4    3    4  1999      7   17          6  \n",
      "1      0    0    0    0  2008      2   14          4  \n",
      "2      0    0    0    0  2013      3    9          6  \n",
      "3     18   12   12    6  2012      2    2          4  \n",
      "4      3    4    3    3  2009      5    9          6  \n",
      "..   ...  ...  ...  ...   ...    ...  ...        ...  \n",
      "132    0    0    0    0  2008      6   25          3  \n",
      "133    0    0    0    0  2006     10   12          4  \n",
      "134    0    0    0    0  2006      7    8          6  \n",
      "135    0    0    0    0  2010     10   29          5  \n",
      "136    0    0    0    0  2009      9    1          2  \n",
      "\n",
      "[137 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "trainval_filename = '../data/train.csv.zip'\n",
    "test_filename = '../data/test.csv.zip'\n",
    "df_trainval, y_trainval = prepare_train_data(trainval_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       City  City Group  Type  P1   P2   P3   P4  P5  P6  P7  ...  P32  P33   \n",
      "0        38           0     0   1  4.0  4.0  4.0   1   2   5  ...    0    0  \\\n",
      "1        27           0     1   3  4.0  4.0  4.0   2   2   5  ...    0    0   \n",
      "2         3           1     0   3  4.0  4.0  4.0   2   2   5  ...    0    0   \n",
      "3        26           0     1   2  4.0  4.0  4.0   2   3   5  ...    0    0   \n",
      "4         1           0     0   2  4.0  4.0  4.0   1   2   5  ...    0    0   \n",
      "...     ...         ...   ...  ..  ...  ...  ...  ..  ..  ..  ...  ...  ...   \n",
      "99995     4           0     0   5  5.0  4.0  4.0   2   2   5  ...    0    0   \n",
      "99996    38           0     1   1  2.0  4.0  3.0   1   1   1  ...    0    0   \n",
      "99997    54           1     1   4  5.0  4.0  4.0   1   2   5  ...    3    2   \n",
      "99998    54           1     0  12  7.5  6.0  6.0   4   4  10  ...    0    4   \n",
      "99999    54           1     1   2  5.0  4.0  4.0   2   2   5  ...    0    2   \n",
      "\n",
      "       P34  P35  P36  P37  Year  Month  Day  week_name  \n",
      "0        0    0    0    0  2011      1   22          6  \n",
      "1        0    0    0    0  2011      3   18          5  \n",
      "2        0    0    0    0  2013     10   30          3  \n",
      "3        0    0    0    0  2013      5    6          1  \n",
      "4        0    0    0    0  2013      7   31          3  \n",
      "...    ...  ...  ...  ...   ...    ...  ...        ...  \n",
      "99995    0    0    0    0  2000      1    5          3  \n",
      "99996    4    0    0    0  2011      7   18          1  \n",
      "99997    4    4    4    2  2012     12   29          6  \n",
      "99998    0    0    0    0  2013     10   12          6  \n",
      "99999    2    4    2    0  2010     10    5          2  \n",
      "\n",
      "[100000 rows x 44 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test = prepare_test_data(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 25176987140096.0000 - val_loss: 30048971128832.0000\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 25176991334400.0000 - val_loss: 30048971128832.0000\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 25176987140096.0000 - val_loss: 30048971128832.0000\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 25176987140096.0000 - val_loss: 30048971128832.0000\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 25176987140096.0000 - val_loss: 30048971128832.0000\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 25176987140096.0000 - val_loss: 30048971128832.0000\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 25176991334400.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 25176987140096.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 25176985042944.0000 - val_loss: 30048969031680.0000\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10000\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 28500870299648.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 28500870299648.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 28500870299648.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 28500870299648.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 28500868202496.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 28500868202496.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500870299648.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500870299648.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 28500868202496.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 28500868202496.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122848722944.0000\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500866105344.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 26/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 27/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 28/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 29/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500866105344.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 30/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500866105344.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 31/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 32/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 28500868202496.0000 - val_loss: 20122846625792.0000\n",
      "Epoch 33/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 28500870299648.0000 - val_loss: 20122846625792.0000\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "Epoch 1/10000\n",
      "1/1 [==============================] - 0s 489ms/step - loss: 27211815976960.0000 - val_loss: 24027930296320.0000\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211815976960.0000 - val_loss: 24027930296320.0000\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 27211818074112.0000 - val_loss: 24027930296320.0000\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211815976960.0000 - val_loss: 24027930296320.0000\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 27211818074112.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 27211818074112.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 22/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 27211813879808.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 23/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 24/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 27211818074112.0000 - val_loss: 24027928199168.0000\n",
      "Epoch 25/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 27211815976960.0000 - val_loss: 24027928199168.0000\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "Epoch 1/10000\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 2/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 3/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 24784859561984.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 4/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 5/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784859561984.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 6/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784859561984.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 7/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784859561984.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 8/10000\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 9/10000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 10/10000\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 24784859561984.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 11/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 12/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 13/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 14/10000\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 24784859561984.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 15/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 16/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 17/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 18/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 19/10000\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 20/10000\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "Epoch 21/10000\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 24784857464832.0000 - val_loss: 31380182401024.0000\n",
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plantator/miniconda3/envs/tf/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/plantator/miniconda3/envs/tf/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/plantator/miniconda3/envs/tf/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/plantator/miniconda3/envs/tf/lib/python3.9/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 332\n",
      "[LightGBM] [Info] Number of data points in the train set: 102, number of used features: 44\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4366984.794118\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's rmse: 2.79678e+06\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's rmse: 2.76872e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's rmse: 2.73529e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's rmse: 2.70805e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's rmse: 2.69223e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's rmse: 2.66282e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's rmse: 2.64506e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's rmse: 2.63429e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's rmse: 2.62025e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's rmse: 2.60388e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's rmse: 2.59307e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's rmse: 2.58971e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's rmse: 2.58136e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's rmse: 2.56908e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's rmse: 2.55642e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's rmse: 2.55879e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's rmse: 2.56362e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's rmse: 2.5527e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's rmse: 2.55099e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's rmse: 2.54568e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's rmse: 2.53497e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's rmse: 2.5304e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's rmse: 2.53601e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's rmse: 2.53057e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's rmse: 2.52284e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's rmse: 2.52129e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's rmse: 2.52872e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's rmse: 2.52535e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's rmse: 2.52027e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's rmse: 2.51222e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's rmse: 2.51118e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's rmse: 2.5177e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's rmse: 2.51623e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's rmse: 2.51579e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's rmse: 2.52042e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's rmse: 2.51838e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's rmse: 2.51254e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's rmse: 2.51976e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's rmse: 2.52889e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's rmse: 2.52601e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's rmse: 2.5242e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's rmse: 2.52016e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's rmse: 2.52716e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's rmse: 2.53412e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's rmse: 2.53434e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's rmse: 2.54195e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's rmse: 2.542e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's rmse: 2.53337e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's rmse: 2.54936e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's rmse: 2.55272e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's rmse: 2.54903e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's rmse: 2.55521e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's rmse: 2.55677e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's rmse: 2.561e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's rmse: 2.55315e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's rmse: 2.56211e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's rmse: 2.56229e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's rmse: 2.56264e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's rmse: 2.57749e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's rmse: 2.5834e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's rmse: 2.58453e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's rmse: 2.58225e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's rmse: 2.58919e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's rmse: 2.5937e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's rmse: 2.59809e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's rmse: 2.59875e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's rmse: 2.60528e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's rmse: 2.60644e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's rmse: 2.59952e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's rmse: 2.59707e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's rmse: 2.59824e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's rmse: 2.59854e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's rmse: 2.60466e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's rmse: 2.60602e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tvalid_0's rmse: 2.6051e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's rmse: 2.60912e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's rmse: 2.61819e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's rmse: 2.624e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's rmse: 2.62547e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's rmse: 2.62412e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's rmse: 2.62557e+06\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's rmse: 2.51118e+06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 324\n",
      "[LightGBM] [Info] Number of data points in the train set: 103, number of used features: 43\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4568715.485437\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's rmse: 1.83991e+06\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's rmse: 1.82364e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's rmse: 1.80935e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's rmse: 1.8026e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's rmse: 1.80493e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's rmse: 1.813e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's rmse: 1.81171e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's rmse: 1.81793e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's rmse: 1.81823e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's rmse: 1.81758e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's rmse: 1.82978e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's rmse: 1.83323e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's rmse: 1.84183e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's rmse: 1.856e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's rmse: 1.86931e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's rmse: 1.87993e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's rmse: 1.89487e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's rmse: 1.89226e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's rmse: 1.90366e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's rmse: 1.93049e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's rmse: 1.92798e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's rmse: 1.94203e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's rmse: 1.95225e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's rmse: 1.95229e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's rmse: 1.95937e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's rmse: 1.96058e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's rmse: 1.97278e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's rmse: 1.96616e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's rmse: 1.97234e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's rmse: 1.97848e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's rmse: 1.98041e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's rmse: 1.99228e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's rmse: 1.99736e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's rmse: 2.00597e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's rmse: 2.01788e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's rmse: 2.02128e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's rmse: 2.03129e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's rmse: 2.03565e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's rmse: 2.04511e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's rmse: 2.03958e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's rmse: 2.05442e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's rmse: 2.07074e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's rmse: 2.07895e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's rmse: 2.09512e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's rmse: 2.09911e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's rmse: 2.10843e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's rmse: 2.12315e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's rmse: 2.12703e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's rmse: 2.14206e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's rmse: 2.15091e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's rmse: 2.15949e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's rmse: 2.17607e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's rmse: 2.17237e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's rmse: 2.17771e+06\n",
      "Early stopping, best iteration is:\n",
      "[4]\tvalid_0's rmse: 1.8026e+06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 322\n",
      "[LightGBM] [Info] Number of data points in the train set: 103, number of used features: 43\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4478220.825243\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's rmse: 2.16802e+06\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's rmse: 2.14334e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's rmse: 2.10386e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's rmse: 2.08321e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's rmse: 2.04334e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's rmse: 2.03061e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's rmse: 1.99795e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's rmse: 1.98953e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's rmse: 1.95096e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's rmse: 1.93013e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's rmse: 1.92036e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's rmse: 1.88651e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's rmse: 1.86807e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's rmse: 1.86149e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's rmse: 1.84792e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's rmse: 1.82835e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's rmse: 1.81813e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's rmse: 1.81142e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's rmse: 1.78707e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's rmse: 1.79122e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's rmse: 1.78663e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's rmse: 1.78209e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's rmse: 1.78935e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's rmse: 1.79463e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's rmse: 1.79737e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's rmse: 1.77727e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's rmse: 1.7743e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's rmse: 1.78044e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's rmse: 1.78044e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's rmse: 1.78023e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's rmse: 1.78171e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's rmse: 1.78069e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's rmse: 1.76733e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's rmse: 1.77157e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's rmse: 1.76445e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's rmse: 1.7665e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's rmse: 1.76724e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's rmse: 1.76822e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's rmse: 1.77442e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's rmse: 1.7674e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's rmse: 1.77986e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's rmse: 1.77628e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's rmse: 1.77912e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's rmse: 1.78e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's rmse: 1.77518e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's rmse: 1.78797e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's rmse: 1.78453e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's rmse: 1.78858e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's rmse: 1.79322e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's rmse: 1.80588e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's rmse: 1.80321e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's rmse: 1.80741e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's rmse: 1.80939e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's rmse: 1.81198e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's rmse: 1.81762e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's rmse: 1.81992e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's rmse: 1.81752e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's rmse: 1.80597e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's rmse: 1.81088e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[60]\tvalid_0's rmse: 1.80755e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's rmse: 1.80968e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's rmse: 1.81472e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[63]\tvalid_0's rmse: 1.82805e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[64]\tvalid_0's rmse: 1.82599e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[65]\tvalid_0's rmse: 1.83087e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's rmse: 1.83411e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[67]\tvalid_0's rmse: 1.83181e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[68]\tvalid_0's rmse: 1.84598e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[69]\tvalid_0's rmse: 1.84499e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's rmse: 1.84959e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[71]\tvalid_0's rmse: 1.83477e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[72]\tvalid_0's rmse: 1.84563e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[73]\tvalid_0's rmse: 1.85063e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[74]\tvalid_0's rmse: 1.85569e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[75]\tvalid_0's rmse: 1.84303e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[76]\tvalid_0's rmse: 1.84612e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[77]\tvalid_0's rmse: 1.83968e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[78]\tvalid_0's rmse: 1.83935e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's rmse: 1.85264e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's rmse: 1.8511e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[81]\tvalid_0's rmse: 1.86083e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[82]\tvalid_0's rmse: 1.86635e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's rmse: 1.8556e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's rmse: 1.85893e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[85]\tvalid_0's rmse: 1.85889e+06\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's rmse: 1.76445e+06\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 318\n",
      "[LightGBM] [Info] Number of data points in the train set: 103, number of used features: 43\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 4399369.106796\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's rmse: 3.14304e+06\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's rmse: 3.14391e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's rmse: 3.14931e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's rmse: 3.12084e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's rmse: 3.13042e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's rmse: 3.10788e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's rmse: 3.09807e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's rmse: 3.07727e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's rmse: 3.10154e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's rmse: 3.08553e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's rmse: 3.10108e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's rmse: 3.0977e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's rmse: 3.11715e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's rmse: 3.12319e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's rmse: 3.13123e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's rmse: 3.13968e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[17]\tvalid_0's rmse: 3.14734e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's rmse: 3.14997e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[19]\tvalid_0's rmse: 3.15804e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's rmse: 3.16529e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[21]\tvalid_0's rmse: 3.16566e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[22]\tvalid_0's rmse: 3.16715e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's rmse: 3.17777e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[24]\tvalid_0's rmse: 3.1807e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[25]\tvalid_0's rmse: 3.1825e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's rmse: 3.16721e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's rmse: 3.17427e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's rmse: 3.1756e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[29]\tvalid_0's rmse: 3.16777e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's rmse: 3.17083e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[31]\tvalid_0's rmse: 3.17458e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's rmse: 3.17629e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's rmse: 3.1772e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's rmse: 3.1804e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's rmse: 3.17079e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's rmse: 3.16845e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's rmse: 3.17204e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[38]\tvalid_0's rmse: 3.16572e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[39]\tvalid_0's rmse: 3.16368e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's rmse: 3.16646e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's rmse: 3.16498e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[42]\tvalid_0's rmse: 3.16768e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's rmse: 3.16877e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[44]\tvalid_0's rmse: 3.17103e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's rmse: 3.17937e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's rmse: 3.17805e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's rmse: 3.18622e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's rmse: 3.1923e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[49]\tvalid_0's rmse: 3.19078e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[50]\tvalid_0's rmse: 3.18399e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[51]\tvalid_0's rmse: 3.17657e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's rmse: 3.16935e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[53]\tvalid_0's rmse: 3.16807e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[54]\tvalid_0's rmse: 3.17029e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[55]\tvalid_0's rmse: 3.17098e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[56]\tvalid_0's rmse: 3.17275e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[57]\tvalid_0's rmse: 3.16954e+06\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[58]\tvalid_0's rmse: 3.16118e+06\n",
      "Early stopping, best iteration is:\n",
      "[8]\tvalid_0's rmse: 3.07727e+06\n",
      " 239/3125 [=>............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-25 15:11:30.438636: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 17600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 2s 614us/step\n"
     ]
    }
   ],
   "source": [
    "model_1a = Model1KNN()\n",
    "model_1b = Model1NN()\n",
    "model_1c = Model1RF()\n",
    "model_1d = Model1lgb()\n",
    "model2 = Model2Linear()\n",
    "models = {\n",
    "    'KNeighbors':model_1a,\n",
    "    'NeuralNetwork':model_1b,\n",
    "    'RandomForest':model_1c, \n",
    "    'LightGBM': model_1d,\n",
    "    'Linear':model2,\n",
    "}\n",
    "\n",
    "lin = fitting_models(models, df_trainval, y_trainval)\n",
    "prediction = predict_revenue(lin ,models, df_test)\n",
    "\n",
    "save_to_cvs(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
